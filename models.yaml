# Snowflake-arctic-embed-l
- repo: Snowflake/snowflake-arctic-embed-l
  name: snowflake-arctic-embed
  kitfile: kitfiles/snowflake-arctic-embed/l-onnx.yml
  modelkit_tag: l-onnx
  model_size: 1GB

# bloom models
- repo: bigscience/bloom-1b1
  name: bloom-1b1
  kitfile: kitfiles/bloom/1b1.yml
  modelkit_tag: safetensor
  model_size: 2.1GB

- repo: bigscience/bloom-1b7
  name: bloom-1b7
  kitfile: kitfiles/bloom/1b7.yml
  modelkit_tag: safetensor
  model_size: 3.4GB

- repo: bigscience/bloom-3b
  name: bloom-3b
  kitfile: kitfiles/bloom/3b.yml
  modelkit_tag: safetensor
  model_size: 6GB

- repo: bigscience/bloom-560m
  name: bloom-560m
  kitfile: kitfiles/bloom/560m.yml
  modelkit_tag: safetensor
  model_size: 1.1GB

# gemma models
- repo: google/gemma-2b
  name: gemma-2b
  family: gemma
  variant: text
  parameterSize: 2b
  description: "2B base version of the Gemma model."
  kitfileTemplate: ./kitfiles/gemma/2b-text-fp16.yml
  fp_precision: f16
  quantizations: ["q8_0", "q5_0", "q4_0"]

- repo: google/gemma-2b-it
  name: gemma-2b
  family: gemma
  variant: instruct
  parameterSize: 2b
  description: "2B instruct version of the Gemma model."
  kitfileTemplate: ./kitfiles/gemma/2b-instruct-fp16.yml
  fp_precision: f16
  quantizations: ["q8_0", "q5_0", "q4_0"]

- repo: google/gemma-7b
  name: gemma-7b
  family: gemma
  variant: text
  parameterSize: 7b
  description: "7B base version of the Gemma model."
  kitfileTemplate: ./kitfiles/gemma/7b-text-fp16.yml
  fp_precision: f16
  quantizations: ["q8_0", "q5_0", "q4_0"]

- repo: google/gemma-7b-it
  name: gemma-7b
  family: gemma
  variant: instruct
  parameterSize: 7b
  description: "7B instruct version of the Gemma model."
  kitfileTemplate: ./kitfiles/gemma/7b-instruct-fp16.yml
  fp_precision: fp16
  quantizations: ["q8_0", "q5_0", "q4_0"]

# Llama-3 models
- repo: meta-llama/Meta-Llama-3-8B
  name: llama3-8b
  parameterSize: 8B
  variant: text
  description: "Llama 3 8B text model"
  kitfileTemplate: ./kitfiles/llama3.yml
  fp_precision: f16
  quantizations: ["q8_0", "q5_0", "q4_0"]

- repo: meta-llama/Meta-Llama-3-8B-Instruct
  name: llama3-8b
  parameterSize: 8B
  variant: instruct
  description: "Llama 3 8B instruct model"
  kitfileTemplate: ./kitfiles/llama3.yml
  fp_precision: f16
  quantizations: ["q8_0", "q5_0", "q4_0"]

- repo: meta-llama/Meta-Llama-3-70B
  name: llama3-70b
  parameterSize: 70B
  variant: text
  description: "Llama 3 70B text model"
  kitfileTemplate: ./kitfiles/llama3.yml
  fp_precision: f16
  quantizations: ["q8_0", "q5_0", "q4_0"]

- repo: meta-llama/Meta-Llama-3-70B-Instruct
  name: llama3-70b
  parameterSize: 70B
  variant: instruct
  description: "Llama 3 70B instruct model"
  kitfileTemplate: ./kitfiles/llama3.yml
  fp_precision: f16
  quantizations: ["q8_0", "q5_0", "q4_0"]

# phi3 models
- repo: microsoft/Phi-3-mini-4k-instruct-gguf
  name: phi3
  kitfile: ./kitfiles/phi3/fp16.yml
  modelkit_tag: 3.8b-mini-instruct-4k-fp16
  model_size: 7.6GB

- repo: microsoft/Phi-3-mini-4k-instruct-gguf
  name: phi3
  kitfile: ./kitfiles/phi3/q4_K_M.yml
  modelkit_tag: 3.8b-mini-instruct-4k-q4_K_M
  model_size: 2.4GB

# Mistral models
- repo: mistralai/Mistral-7B-v0.1
  name: mistral-7b
  parameterSize: 7B
  variant: text
  description: "Mistral 7B v.0.1 text model"
  kitfileTemplate: ./kitfiles/mistral/7b-v0.1.yml
  fp_precision: f16
  quantizations: ["q8_0", "q5_0", "q4_0"]

- repo: mistralai/Mistral-7B-Instruct-v0.1
  name: mistral-7b
  parameterSize: 7B
  variant: instruct
  description: "Mistral 7B v.0.1 instruct model"
  kitfileTemplate: ./kitfiles/mistral/7b-v0.1.yml
  fp_precision: f16
  quantizations: ["q8_0", "q5_0", "q4_0"]

- repo: mistralai/Mistral-7B-v0.3
  name: mistral-7b
  parameterSize: 7B
  variant: text
  description: "Mistral 7B v.0.3 text model"
  kitfileTemplate: ./kitfiles/mistral/7b-v0.3.yml
  fp_precision: f16
  quantizations: ["q8_0", "q5_0", "q4_0"]

- repo: mistralai/Mistral-7B-Instruct-v0.3
  name: mistral-7b
  parameterSize: 7B
  variant: instruct
  description: "Mistral 7B v.0.3 instruct model"
  kitfileTemplate: ./kitfiles/mistral/7b-v0.3.yml
  fp_precision: f16
  quantizations: ["q8_0", "q5_0", "q4_0"]

# Mixtral models
- repo: mistralai/Mixtral-8x7B-v0.1
  name: mixtral-8x7b
  parameterSize: 47B
  variant: text
  description: "Mixtral 8x7B v.0.1 text model"
  kitfileTemplate: ./kitfiles/mixtral/8x7b-v0.1.yml
  fp_precision: f16
  quantizations: ["q8_0", "q5_0", "q4_0"]

- repo: mistralai/Mixtral-8x7B-Instruct-v0.1
  name: mixtral-8x7b
  parameterSize: 47B
  variant: instruct
  description: "Mixtral 8x7B v.0.1 instruct model"
  kitfileTemplate: ./kitfiles/mixtral/8x7b-v0.1.yml
  fp_precision: f16
  quantizations: ["q8_0", "q5_0", "q4_0"]

- repo: mistralai/Mixtral-8x22B-v0.1
  name: mixtral-8x22b
  parameterSize: 141B
  variant: text
  description: "Mixtral 8x22B v.0.1 text model"
  kitfileTemplate: ./kitfiles/mixtral/8x22b-v0.1.yml
  fp_precision: f16
  quantizations: ["q8_0", "q5_0", "q4_0"]

- repo: mistralai/Mixtral-8x22B-Instruct-v0.1
  name: mixtral-8x22b
  parameterSize: 141B
  variant: instruct
  description: "Mixtral 8x22B v.0.1 instruct model"
  kitfileTemplate: ./kitfiles/mixtral/8x22b-v0.1.yml
  fp_precision: f16
  quantizations: ["q8_0", "q5_0", "q4_0"]

# Qwen2 models
- repo: Qwen/Qwen2-0.5B-Instruct-GGUF
  name: qwen2-0.5b
  kitfile: ./kitfiles/qwen2-0.5B/0.5b-instruct-fp16.yml
  modelkit_tag: 0.5b-instruct-fp16
  model_size: 0.9GB

- repo: Qwen/Qwen2-0.5B-Instruct-GGUF
  name: qwen2-0.5b
  kitfile: ./kitfiles/qwen2-0.5B/0.5b-instruct-q2_K.yml
  modelkit_tag: 0.5b-instruct-q2_K
  model_size: 0.3GB

- repo: Qwen/Qwen2-0.5B-Instruct-GGUF
  name: qwen2-0.5b
  kitfile: ./kitfiles/qwen2-0.5B/0.5b-instruct-q3_K_M.yml
  modelkit_tag: 0.5b-instruct-q3_K_M
  model_size: 0.3GB

- repo: Qwen/Qwen2-0.5B-Instruct-GGUF
  name: qwen2-0.5b
  kitfile: ./kitfiles/qwen2-0.5B/0.5b-instruct-q4.yml
  modelkit_tag: 0.5b-instruct-q4_0
  model_size: 0.4GB

- repo: Qwen/Qwen2-0.5B-Instruct-GGUF
  name: qwen2-0.5b
  kitfile: ./kitfiles/qwen2-0.5B/0.5b-instruct-q4_K_M.yml
  modelkit_tag: 0.5b-instruct-q4_K_M
  model_size: 0.4GB

- repo: Qwen/Qwen2-0.5B-Instruct-GGUF
  name: qwen2-0.5b
  kitfile: ./kitfiles/qwen2-0.5B/0.5b-instruct-q5.yml
  modelkit_tag: 0.5b-instruct-q5_0
  model_size: 0.4GB

- repo: Qwen/Qwen2-0.5B-Instruct-GGUF
  name: qwen2-0.5b
  kitfile: ./kitfiles/qwen2-0.5B/0.5b-instruct-q5_K_M.yml
  modelkit_tag: 0.5b-instruct-q5_K_M
  model_size: 0.4GB

- repo: Qwen/Qwen2-0.5B-Instruct-GGUF
  name: qwen2-0.5b
  kitfile: ./kitfiles/qwen2-0.5B/0.5b-instruct-q6_K.yml
  modelkit_tag: 0.5b-instruct-q6_K
  model_size: 0.5GB

- repo: Qwen/Qwen2-0.5B-Instruct-GGUF
  name: qwen2-0.5b
  kitfile: ./kitfiles/qwen2-0.5B/0.5b-instruct-q8.yml
  modelkit_tag: 0.5b-instruct-q8_0
  model_size: 0.5GB

- repo: Qwen/Qwen2-7B-Instruct-GGUF
  name: qwen2-7b
  kitfile: ./kitfiles/qwen2-7B/7b-instruct-fp16.yml
  modelkit_tag: 7b-instruct-fp16
  model_size: 50GB
  
- repo: Qwen/Qwen2-7B-Instruct-GGUF
  name: qwen2-7b
  kitfile: ./kitfiles/qwen2-7B/7b-instruct-q2_K.yml
  modelkit_tag: 7b-instruct-q2_K
  model_size: 50GB

- repo: Qwen/Qwen2-7B-Instruct-GGUF
  name: qwen2-7b
  kitfile: ./kitfiles/qwen2-7B/7b-instruct-q3_K_M.yml
  modelkit_tag: 7b-instruct-q3_K_M
  model_size: 50GB

- repo: Qwen/Qwen2-7B-Instruct-GGUF
  name: qwen2-7b
  kitfile: ./kitfiles/qwen2-7B/7b-instruct-q4.yml
  modelkit_tag: 7b-instruct-q4_0
  model_size: 50GB

- repo: Qwen/Qwen2-7B-Instruct-GGUF
  name: qwen2-7b
  kitfile: ./kitfiles/qwen2-7B/7b-instruct-q4_K_M.yml
  modelkit_tag: 7b-instruct-q4_K_M
  model_size: 50GB

- repo: Qwen/Qwen2-7B-Instruct-GGUF
  name: qwen2-7b
  kitfile: ./kitfiles/qwen2-7B/7b-instruct-q5.yml
  modelkit_tag: 7b-instruct-q5_0
  model_size: 50GB

- repo: Qwen/Qwen2-7B-Instruct-GGUF
  name: qwen2-7b
  kitfile: ./kitfiles/qwen2-7B/7b-instruct-q5_K_M.yml
  modelkit_tag: 7b-instruct-q5_K_M
  model_size: 50GB

- repo: Qwen/Qwen2-7B-Instruct-GGUF
  name: qwen2-7b
  kitfile: ./kitfiles/qwen2-7B/7b-instruct-q6_K.yml
  modelkit_tag: 7b-instruct-q6_K
  model_size: 50GB

- repo: Qwen/Qwen2-7B-Instruct-GGUF
  name: qwen2-7b
  kitfile: ./kitfiles/qwen2-7B/7b-instruct-q8.yml
  modelkit_tag: 7b-instruct-q8_0
  model_size: 50GB

# sentence-transformers model
- repo: sentence-transformers/all-MiniLM-L6-v2
  name: all-minilm-l6-v2
  kitfile: kitfiles/all-minilm-L6-v2/safetensor.yml
  modelkit_tag: safetensor
  model_size: 1GB

# falcon models
- repo: tiiuae/falcon-7b
  name: falcon-7b
  kitfile: kitfiles/falcon/7b-text.yml
  modelkit_tag: 7b-text
  model_size: 15GB

- repo: tiiuae/falcon-7b-instruct
  name: falcon-7b
  kitfile: kitfiles/falcon/7b-instruct.yml
  modelkit_tag: 7b-instruct
  model_size: 15GB

- repo: tiiuae/falcon-40b
  name: falcon-40b
  kitfile: kitfiles/falcon/40b-text.yml
  modelkit_tag: 40b-text
  model_size: 90GB

- repo: tiiuae/falcon-40b-instruct
  name: falcon-40b
  kitfile: kitfiles/falcon/40b-instruct.yml
  modelkit_tag: 40b-instruct
  model_size: 90GB

# MMLU Dataset
- repo: cais/mmlu
  name: mmlu-dataset
  kitfile: kitfiles/mmlu-dataset/mmlu.yml
  modelkit_tag: latest
  model_size: 0.2GB
